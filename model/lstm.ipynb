{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster Classification using LSTM with PyTorch\n",
    "\n",
    "This notebook builds a disaster classification model using LSTM neural networks to classify tweets into one of these categories:\n",
    "- 0: No disaster\n",
    "- 1: Earthquake\n",
    "- 2: Flood\n",
    "- 3: Hurricane\n",
    "- 4: Tornado\n",
    "- 5: Wildfire\n",
    "\n",
    "We'll use PyTorch to implement the LSTM model and perform the classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "# %pip install seaborn scikit-learn tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed dataset with specified encoding\n",
    "df = pd.read_csv('../data-preprocessing/combined_dataset.csv', encoding='utf-16')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "class_counts = df['label'].value_counts().sort_index()\n",
    "disaster_types = ['No Disaster', 'Earthquake', 'Flood', 'Hurricane', 'Tornado', 'Wildfire']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values)\n",
    "plt.xticks(class_counts.index, disaster_types, rotation=45)\n",
    "plt.xlabel('Disaster Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Disaster Types')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text function\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Fill missing values with an empty string\n",
    "df['text'] = df['text'].fillna('')\n",
    "\n",
    "# Apply cleaning to the text column\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Display sample of cleaned texts\n",
    "pd.DataFrame({\n",
    "    'Original': df['text'].head(),\n",
    "    'Cleaned': df['cleaned_text'].head()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Vocabulary Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "def build_vocabulary(texts, max_vocab_size=10000):\n",
    "    word_counts = Counter()\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        word_counts.update(words)\n",
    "    \n",
    "    # Get most common words\n",
    "    vocab = ['<PAD>', '<UNK>'] + [word for word, _ in word_counts.most_common(max_vocab_size-2)]\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    return vocab, word_to_idx\n",
    "\n",
    "# Tokenize text\n",
    "def tokenize(text, word_to_idx, max_length=50):\n",
    "    words = text.split()\n",
    "    tokens = [word_to_idx.get(word, word_to_idx['<UNK>']) for word in words]\n",
    "    \n",
    "    # Pad or truncate to fixed length\n",
    "    if len(tokens) < max_length:\n",
    "        tokens = tokens + [word_to_idx['<PAD>']] * (max_length - len(tokens))\n",
    "    else:\n",
    "        tokens = tokens[:max_length]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Build vocabulary from the cleaned texts\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "vocab, word_to_idx = build_vocabulary(df['cleaned_text'], MAX_VOCAB_SIZE)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Tokenize texts\n",
    "df['tokens'] = df['cleaned_text'].apply(lambda x: tokenize(x, word_to_idx, MAX_LENGTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Train, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels\n",
    "X = np.array(df['tokens'].tolist())\n",
    "y = np.array(df['label'])\n",
    "\n",
    "# First split: 80% train+val, 20% test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Second split: 75% train, 25% validation (from the 80% train+val)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42, stratify=y_train_val)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Plot distribution of disaster types in training, validation, and test sets\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# Training set distribution\n",
    "train_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "axes[0].bar(train_counts.index, train_counts.values)\n",
    "axes[0].set_xticks(train_counts.index)\n",
    "axes[0].set_xticklabels(disaster_types, rotation=45)\n",
    "axes[0].set_title('Training Set')\n",
    "axes[0].set_xlabel('Disaster Type')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Validation set distribution\n",
    "val_counts = pd.Series(y_val).value_counts().sort_index()\n",
    "axes[1].bar(val_counts.index, val_counts.values)\n",
    "axes[1].set_xticks(val_counts.index)\n",
    "axes[1].set_xticklabels(disaster_types, rotation=45)\n",
    "axes[1].set_title('Validation Set')\n",
    "axes[1].set_xlabel('Disaster Type')\n",
    "\n",
    "# Test set distribution\n",
    "test_counts = pd.Series(y_test).value_counts().sort_index()\n",
    "axes[2].bar(test_counts.index, test_counts.values)\n",
    "axes[2].set_xticks(test_counts.index)\n",
    "axes[2].set_xticklabels(disaster_types, rotation=45)\n",
    "axes[2].set_title('Test Set')\n",
    "axes[2].set_xlabel('Disaster Type')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DisasterDataset(X_train, y_train)\n",
    "val_dataset = DisasterDataset(X_val, y_val)\n",
    "test_dataset = DisasterDataset(X_test, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout if n_layers > 1 else 0,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.fc = nn.Linear(fc_input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text shape: [batch size, sentence length]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        embedded = self.embedding(text)  # shape: [batch size, sentence length, embedding dim]\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Extract the final forward and backward hidden states if bidirectional\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1,:,:]\n",
    "        \n",
    "        # Apply dropout\n",
    "        hidden = self.dropout(hidden)\n",
    "        \n",
    "        # Pass through linear layer\n",
    "        output = self.fc(hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model and Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model hyperparameters\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 6  # Number of disaster classes\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = word_to_idx['<PAD>']\n",
    "\n",
    "# Initialize model\n",
    "model = LSTMClassifier(\n",
    "    VOCAB_SIZE, \n",
    "    EMBEDDING_DIM, \n",
    "    HIDDEN_DIM, \n",
    "    OUTPUT_DIM, \n",
    "    N_LAYERS, \n",
    "    BIDIRECTIONAL, \n",
    "    DROPOUT, \n",
    "    PAD_IDX\n",
    ")\n",
    "\n",
    "# Move model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n",
    "print(f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # Progress bar for training\n",
    "    progress_bar = tqdm(iterator, desc='Training')\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Get batch data\n",
    "        text, labels = batch\n",
    "        text = text.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(text)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predicted_classes = predictions.argmax(dim=1)\n",
    "        correct = (predicted_classes == labels).float().sum()\n",
    "        acc = correct / len(labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update progress bar\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        progress_bar.set_postfix({'loss': epoch_loss / (progress_bar.n + 1), 'acc': epoch_acc / (progress_bar.n + 1)})\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # No gradient calculation needed for evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(iterator, desc='Evaluating'):\n",
    "            # Get batch data\n",
    "            text, labels = batch\n",
    "            text = text.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(text)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted_classes = predictions.argmax(dim=1)\n",
    "            correct = (predicted_classes == labels).float().sum()\n",
    "            acc = correct / len(labels)\n",
    "            \n",
    "            # Update total loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            # Save predictions and labels for classification report\n",
    "            all_predictions.extend(predicted_classes.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of epochs\n",
    "N_EPOCHS = 10\n",
    "PATIENCE = 3  # Number of epochs to wait for improvement before stopping\n",
    "\n",
    "# Initialize variables to store best validation accuracy\n",
    "best_valid_acc = 0.0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Lists to store metrics for plotting\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
    "    \n",
    "    # Training phase\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Evaluation phase\n",
    "    val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f\"\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"\\tVal Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_acc > best_valid_acc:\n",
    "        best_valid_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_disaster_model.pt')\n",
    "        print(f\"\\tBest validation accuracy so far! Model saved.\")\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"\\tNo improvement in validation accuracy for {epochs_no_improve} epoch(s).\")\n",
    "    \n",
    "    # Check early stopping condition\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "        break\n",
    "    \n",
    "    print(\"--------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the LSTM and embedding layers\n",
    "for param in model.embedding.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.lstm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Print the model to verify which parameters are frozen\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {'requires_grad=True' if param.requires_grad else 'requires_grad=False'}\")\n",
    "    \n",
    "# Fine-tune the model with the remaining parameters\n",
    "N_FINE_TUNE_EPOCHS = 5\n",
    "\n",
    "# Initialize variables to store best validation accuracy during fine-tuning\n",
    "best_fine_tune_acc = 0.0\n",
    "\n",
    "# Lists to store metrics for plotting\n",
    "fine_tune_train_losses = []\n",
    "fine_tune_train_accs = []\n",
    "fine_tune_val_losses = []\n",
    "fine_tune_val_accs = []\n",
    "\n",
    "for epoch in range(N_FINE_TUNE_EPOCHS):\n",
    "    print(f\"Fine-tuning Epoch {epoch+1}/{N_FINE_TUNE_EPOCHS}\")\n",
    "    \n",
    "    # Training phase\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    fine_tune_train_losses.append(train_loss)\n",
    "    fine_tune_train_accs.append(train_acc)\n",
    "    \n",
    "    # Evaluation phase\n",
    "    val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "    fine_tune_val_losses.append(val_loss)\n",
    "    fine_tune_val_accs.append(val_acc)\n",
    "    \n",
    "    print(f\"\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"\\tVal Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Save the best model during fine-tuning\n",
    "    if val_acc > best_fine_tune_acc and  val_acc > best_valid_acc:\n",
    "        best_fine_tune_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_disaster_model.pt')\n",
    "        print(f\"\\tBest validation accuracy so far during fine-tuning! Model saved.\")\n",
    "    else:\n",
    "        print(f\"\\tNo improvement in validation accuracy during fine-tuning.\")\n",
    "    \n",
    "    print(\"--------------------------------------------------\")\n",
    "    \n",
    "if best_fine_tune_acc > best_valid_acc:\n",
    "    print(\"Best model is the fine-tuned model.\")\n",
    "else:\n",
    "    print(\"Best model is the model before fine-tuning.\")\n",
    "\n",
    "# Unfreeze the LSTM and embedding layers\n",
    "for param in model.embedding.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.lstm.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Print the model to verify which parameters are unfrozen\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {'requires_grad=True' if param.requires_grad else 'requires_grad=False'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training and Validation Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves for training and validation\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, N_EPOCHS+1), train_losses, 'b-', label='Training Loss')\n",
    "plt.plot(range(1, N_EPOCHS+1), val_losses, 'r-', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, N_EPOCHS+1), [acc * 100 for acc in train_accs], 'b-', label='Training Accuracy')\n",
    "plt.plot(range(1, N_EPOCHS+1), [acc * 100 for acc in val_accs], 'r-', label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "# Plot loss curves for fine-tuning\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, N_FINE_TUNE_EPOCHS+1), fine_tune_train_losses, 'b-', label='Fine-tuning Training Loss')\n",
    "plt.plot(range(1, N_FINE_TUNE_EPOCHS+1), fine_tune_val_losses, 'r-', label='Fine-tuning Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Fine-tuning Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, N_FINE_TUNE_EPOCHS+1), [acc * 100 for acc in fine_tune_train_accs], 'b-', label='Fine-tuning Training Accuracy')\n",
    "plt.plot(range(1, N_FINE_TUNE_EPOCHS+1), [acc * 100 for acc in fine_tune_val_accs], 'r-', label='Fine-tuning Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Fine-tuning Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_disaster_model.pt'))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_preds, test_labels = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_labels, test_preds, target_names=disaster_types))\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=disaster_types, yticklabels=disaster_types)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Function for New Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_disaster(text, model, word_to_idx, max_length=50):\n",
    "    # Clean and tokenize text\n",
    "    cleaned_text = clean_text(text)\n",
    "    tokens = tokenize(cleaned_text, word_to_idx, max_length)\n",
    "    \n",
    "    # Convert to tensor and add batch dimension\n",
    "    tokens_tensor = torch.tensor([tokens], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        predicted_class = outputs.argmax(dim=1).item()\n",
    "        confidence = probabilities[0][predicted_class].item()\n",
    "    \n",
    "    return {\n",
    "        'text': text,\n",
    "        'predicted_class': predicted_class,\n",
    "        'disaster_type': disaster_types[predicted_class],\n",
    "        'confidence': confidence * 100,\n",
    "        'probabilities': {disaster_types[i]: prob.item() * 100 for i, prob in enumerate(probabilities[0])}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Example Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with some example tweets\n",
    "example_tweets = [\n",
    "    \"Earthquake just hit the city! Buildings are shaking and people are running outside.\",\n",
    "    \"The river has overflowed and many streets are now underwater. #flooding\",\n",
    "    \"Hurricane warning in effect for the coastal areas. Everyone please evacuate.\",\n",
    "    \"Just saw a tornado touch down near the highway. It's moving east quickly.\",\n",
    "    \"The wildfire is spreading rapidly due to strong winds. Fire crews are responding.\",\n",
    "    \"Just had a great lunch at the new restaurant downtown. Would recommend!\"\n",
    "]\n",
    "\n",
    "for tweet in example_tweets:\n",
    "    result = predict_disaster(tweet, model, word_to_idx)\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Predicted: {result['disaster_type']} (Class {result['predicted_class']})\")\n",
    "    print(f\"Confidence: {result['confidence']:.2f}%\")\n",
    "    print(\"Probabilities:\")\n",
    "    for disaster_type, prob in result['probabilities'].items():\n",
    "        print(f\"  {disaster_type}: {prob:.2f}%\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and Vocabulary for Future Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Save model and vocabulary for future use\n",
    "\n",
    "# Create directory for model artifacts\n",
    "MODEL_DIR = 'disaster_model'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Save model state\n",
    "torch.save(model, os.path.join(MODEL_DIR, 'model.pt'))\n",
    "\n",
    "# Save model configuration for reconstruction\n",
    "model_config = {\n",
    "    'vocab_size': VOCAB_SIZE,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'output_dim': OUTPUT_DIM,\n",
    "    'n_layers': N_LAYERS,\n",
    "    'bidirectional': BIDIRECTIONAL,\n",
    "    'dropout': DROPOUT,\n",
    "    'pad_idx': PAD_IDX,\n",
    "    'max_length': MAX_LENGTH\n",
    "}\n",
    "\n",
    "# Save vocabulary and configurations\n",
    "with open(os.path.join(MODEL_DIR, 'vocab.pkl'), 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, 'word_to_idx.pkl'), 'wb') as f:\n",
    "    pickle.dump(word_to_idx, f)\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, 'model_config.json'), 'w') as f:\n",
    "    json.dump(model_config, f)\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, 'disaster_types.json'), 'w') as f:\n",
    "    json.dump(disaster_types, f)\n",
    "\n",
    "print(f\"Model and vocabulary saved to {MODEL_DIR}/\")\n",
    "\n",
    "# Additional evaluation metrics - Per-class F1 scores\n",
    "\n",
    "# Calculate per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(test_labels, test_preds)\n",
    "\n",
    "# Create a dataframe with per-class metrics\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Disaster Type': disaster_types,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1 Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "print(\"\\nPer-class metrics:\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Plot F1 scores\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.bar(disaster_types, f1)\n",
    "# plt.title('F1 Scores by Disaster Type')\n",
    "# plt.xlabel('Disaster Type')\n",
    "# plt.ylabel('F1 Score')\n",
    "# plt.ylim(0, 1.0)\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# Class imbalance handling - Calculate class weights\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "\n",
    "print(\"\\nClass weights to handle imbalance:\")\n",
    "for i, weight in enumerate(class_weights):\n",
    "    print(f\"{disaster_types[i]}: {weight:.4f}\")\n",
    "\n",
    "# Create a deployable prediction pipeline\n",
    "class DisasterPredictor:\n",
    "    def __init__(self, model_dir='disaster_model'):\n",
    "        # Load configuration\n",
    "        with open(os.path.join(model_dir, 'model_config.json'), 'r') as f:\n",
    "            self.config = json.load(f)\n",
    "        \n",
    "        # Load vocabulary\n",
    "        with open(os.path.join(model_dir, 'vocab.pkl'), 'rb') as f:\n",
    "            self.vocab = pickle.load(f)\n",
    "            \n",
    "        with open(os.path.join(model_dir, 'word_to_idx.pkl'), 'rb') as f:\n",
    "            self.word_to_idx = pickle.load(f)\n",
    "            \n",
    "        with open(os.path.join(model_dir, 'disaster_types.json'), 'r') as f:\n",
    "            self.disaster_types = json.load(f)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = LSTMClassifier(\n",
    "            self.config['vocab_size'], \n",
    "            self.config['embedding_dim'], \n",
    "            self.config['hidden_dim'], \n",
    "            self.config['output_dim'], \n",
    "            self.config['n_layers'], \n",
    "            self.config['bidirectional'], \n",
    "            self.config['dropout'], \n",
    "            self.config['pad_idx']\n",
    "        )\n",
    "        \n",
    "        # Load model weights\n",
    "        self.model.load_state_dict(torch.load(\n",
    "            os.path.join(model_dir, 'model.pt'),\n",
    "            map_location=self.device\n",
    "        ))\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        max_length = self.config['max_length']\n",
    "        words = text.split()\n",
    "        tokens = [self.word_to_idx.get(word, self.word_to_idx['<UNK>']) for word in words]\n",
    "        \n",
    "        # Pad or truncate\n",
    "        if len(tokens) < max_length:\n",
    "            tokens = tokens + [self.word_to_idx['<PAD>']] * (max_length - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:max_length]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def predict(self, text):\n",
    "        # Clean and tokenize\n",
    "        cleaned_text = self.clean_text(text)\n",
    "        tokens = self.tokenize(cleaned_text)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        tensor = torch.LongTensor([tokens]).to(self.device)\n",
    "        \n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(tensor)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            predicted_class = outputs.argmax(dim=1).item()\n",
    "            confidence = probabilities[0][predicted_class].item()\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'predicted_class': predicted_class,\n",
    "            'disaster_type': self.disaster_types[predicted_class],\n",
    "            'confidence': confidence * 100,\n",
    "            'probabilities': {self.disaster_types[i]: prob.item() * 100 for i, prob in enumerate(probabilities[0])}\n",
    "        }\n",
    "\n",
    "# Test the predictor pipeline\n",
    "print(\"\\nTesting prediction pipeline...\")\n",
    "predictor = DisasterPredictor()\n",
    "\n",
    "# Test examples with the pipeline\n",
    "for tweet in example_tweets:\n",
    "    result = predictor.predict(tweet)\n",
    "    print(f\"\\nText: {result['text']}\")\n",
    "    print(f\"Predicted: {result['disaster_type']} (Class {result['predicted_class']})\")\n",
    "    print(f\"Confidence: {result['confidence']:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
